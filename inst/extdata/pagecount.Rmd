---
title: "Pagecount preprocessing summary"
author: "`r author`"
date: "`r Sys.Date()`"
output: markdown_document
---

```{r init, echo=FALSE}
#opts_chunk$set(comment=NA, fig.width=6, fig.height=6)
opts_chunk$set(fig.path = "figure/pagecount-")
df <- df.preprocessed
```


## Page counts

  * Page count available for `r sum(!is.na(df$pagecount))` documents in total (`r round(100*(sum(!is.na(df$pagecount)))/nrow(df), 1)`%), including both readily available and estimated page counts.

  * Page count readily available for `r sum(!is.na(df$pagecount.orig))` documents (`r round(100*(sum(!is.na(df$pagecount.orig)))/nrow(df), 1)`%). 

  * Page count estimated for `r sum(!is.na(df$pagecount)) - sum(!is.na(df$pagecount.orig))` documents (`r round(100*(sum(!is.na(df$pagecount)) - sum(!is.na(df$pagecount.orig)))/nrow(df), 1)`%).

  * Page count missing and could not be estimated for `r sum(is.na(df.preprocessed$pagecount.orig) & is.na(df.preprocessed$pagecount))` documents (`r round(100 * sum(is.na(df.preprocessed$pagecount.orig) & is.na(df.preprocessed$pagecount))/nrow(df.preprocessed), 1)`%).

  * Page count updated for `r sum(!df$pagecount.orig == df$pagecount, na.rm = T)` documents in the validation phase.
  
  * [Conversions from raw data to final page count estimates](output.tables/pagecount_conversions.csv)

  * [Augmented pagecounts](output.tables/pagecount_discarded.csv) For these cases the page count is missing (or discarded) in the original data, and estimated based on median page counts for [single volume](mean_pagecounts_singlevol.csv), [multi-volume](mean_pagecounts_multivol.csv) and [issues](mean_pagecounts_issue.csv), calculated from those documents where page count info was available.

  * [Automated unit tests for page count conversions](https://github.com/COMHIS/bibliographica/blob/master/inst/extdata/tests_polish_physical_extent.csv) - these are used to control that the page count conversions remain correct when changes are made to the cleanup routines



## Average page counts

Mean and median page counts calculated based on the documents where
the page count information was readily available. Also see the
correponding numerical tables with page count estimates:

 * [Single volume](mean_pagecounts_singlevol.csv)
 * [Multi-volume](mean_pagecounts_multivol.csv)
 * [Issue](mean_pagecounts_issue.csv)

These estimates are used to fill in page count info for the remaining
documents where page count info is missing.

The multi-volume documents average page counts are given per volume.

The page count estimates are calculated without plates. Plate
information is added separately for each document on top of the page
count estimate.

```{r size-pagecountsmulti, echo=FALSE, message=FALSE, warning=FALSE}
mean.pagecounts <- get_mean_pagecounts(df, exclude.plates = TRUE)
```

```{r size-pagecountsmulti2, echo=FALSE, message=FALSE, warning=FALSE, fig.width=25, fig.height=5}
theme_set(theme_bw(20))
dfs <- NULL
for (doctype in names(mean.pagecounts)) {
  mpc <- as.data.frame(mean.pagecounts[[doctype]])
  mpc$doctype <- rep(doctype, nrow(mpc))
  dfs <- rbind(dfs, mpc)
}
dfs$doctype <- gsub("multivol", "multi-volume", dfs$doctype)
dfs$doctype <- gsub("singlevol", "single volume", dfs$doctype)
dfs$doctype <- factor(dfs$doctype, levels = c("single volume", "multi-volume", "issue"))
colnames(dfs) <- gsub("\\.pages", "", colnames(dfs))

dfm <- melt(dfs, id = c("doctype", "doc.dimension", "n"))
dfm <- dfm[!dfm$doc.dimension == "NA", ]
dfm$doc.dimension <- droplevels(dfm$doc.dimension)
dfm <- filter(dfm, !is.na(value))

p <- ggplot(dfm, aes(color = doctype, shape = variable, y = value, x = doc.dimension))
p <- p + geom_point(stat = "identity", position = "dodge", size = 5)
p <- p + ylab("Pages")
p <- p + xlab("")
p <- p + coord_flip()
p <- p + ggtitle(paste("Estimated page counts"))
#p <- p + scale_size_continuous(limits=range(dfm$n), range=c(3, 10))
lims <- range(na.omit(c(dfs$mean, dfs$median)))
pics <- list()
pics[[1]] <- p
for (doct in unique(dfs$doctype)) {
  dfm <- subset(dfs, doctype == doct)
  dfm <- melt(dfm, id = c("doctype", "doc.dimension", "n"))
  dfm <- filter(dfm, !is.na(value))  
  #dfm <- dfm[!dfm$doc.dimension == "NA", ]
  #dfm$doc.dimension <- droplevels(dfm$doc.dimension)
  
  p <- ggplot(dfm, aes(fill = variable, y = value, x = doc.dimension))
  p <- p + geom_bar(stat = "identity", position = "dodge")
  p <- p + ylab("Pages")
  p <- p + xlab("")
  p <- p + coord_flip()
  p <- p + ggtitle(doct)
  p <- p + ylim(0, max(lims))
  
  pics[[doct]] <- p

}

suppressWarnings(grid.arrange(pics[[1]], pics[[2]], pics[[3]], pics[[4]], nrow = 1))

#kable(mean.pagecounts$singlevol, caption = "Estimated page counts (single volume)", digits = 2)
#kable(mean.pagecounts$multivol, caption = "Estimated page counts (multi-volume)", digits = 2)
#kable(mean.pagecounts$issue, caption = "Estimated page counts (issues)", digits = 2)
```



### Document size distribution

```{r pagecountstat, echo=FALSE, message=FALSE, warning=FALSE, fig.width=12, fig.height=5}
df <- df.preprocessed

  theme_set(theme_bw(15))

  # Single-volume docs
  dff <- filter(df, volcount == 1 & is.na(volnumber))

  dff2 <- dff %>% group_by(gatherings, 
		    	     pagecount) %>%
			     tally()

  dff3 <- dff %>% group_by(gatherings) %>%
		    summarize(mean = mean(pagecount, na.rm = T), 
		              median = median(pagecount, na.rm = T))

  p <- ggplot(dff2, aes(y = gatherings, x = pagecount)) 
  p <- p + geom_point(aes(size = n))
  p <- p + geom_point(data = dff3, aes(y = gatherings, x = mean), col = "red", size = 3)
  p <- p + geom_point(data = dff3, aes(y = gatherings, x = median), col = "blue", size = 3)
  p <- p + scale_x_log10(breaks = c(1, 10, 100, 1000))
  p <- p + xlab("Total page count (blue: median; red: mean)")
  p <- p + ylab("Document size")
  p <- p + ggtitle(paste("Pages: single-volume documents (n=", nrow(dff), ")", sep = ""))
  p1 <- p 

  # Multi-volume docs
  theme_set(theme_bw(15))
  dff <- filter(df, 
 	   (volcount > 1 | 
	   #(items == 1 & !is.na(volnumber))) # include when items info is again available
	   (!is.na(volnumber)))	   
	   #pagecount > 10
	   )
  dff2 <- dff %>% group_by(gatherings, 
		    	     pagecount) %>%
		    	     tally()

  dff3 <- dff %>% group_by(gatherings) %>%
		    summarize(mean = mean(pagecount, na.rm = T), 
		     median = median(pagecount, na.rm = T))

  p <- ggplot(dff2, aes(y = gatherings, x = pagecount)) 
  p <- p + geom_point(aes(size = n))
  p <- p + geom_point(data = dff3, aes(y = gatherings, x = mean), col = "red", size = 3)
  p <- p + geom_point(data = dff3, aes(y = gatherings, x = median), col = "blue", size = 3)
  p <- p + scale_x_log10(breaks = c(1, 10, 100, 1000))
  p <- p + xlab("Total page count (blue: median; red: mean)")
  p <- p + ylab("Document size")
  p <- p + ggtitle(paste("Pages: multi-volume documents (n=", nrow(dff), ")", sep = ""))
  p2 <- p 

library(gridExtra)
grid.arrange(p1, p2, nrow = 1)
```

Left: Gatherings vs. overall pagecounts (original + estimated). Right: Only the estimated page counts (for the `r sum(!is.na(df$pagecount)) - sum(!is.na(df$pagecount.orig))` documents that have missing pagecount info in the original data):

```{r size-estimated, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=6, fig.show="hold", out.width="430px"}
theme_set(theme_bw(20))
dfs <- select(df, pagecount, gatherings) 
dfs$pagecount <- as.numeric(gsub(" pages", "", dfs$pagecount))
dfs <- dfs %>% filter(!is.na(pagecount) & !is.na(gatherings))
dfg <- group_by(dfs, pagecount, gatherings) %>% tally()
names(dfg) <- c("pages", "gatherings", "documents")
dfg$gatherings <- factor(dfg$gatherings, levels = levels(df$gatherings))
ylims <- range(dfg$pages)
p <- ggplot(dfg, aes(x = gatherings, y = pages)) 
#p <- p + scale_y_continuous(trans = "log10")
n <- 1 + nchar(max(na.omit(table(dfg$pages))))
ylim <- ylim(ylims)
p <- p + scale_y_log10(breaks=10^(0:n))
p <- p + geom_point(aes(size = documents))
p <- p + scale_size(trans="log10")
p <- p + ggtitle(paste("Estimated and original pages\n(n=", sum(dfg$documents), ")", sep = ""))
p <- p + xlab("Size (gatherings)")
p <- p + ylab("Pages (original and estimated)")
p <- p + coord_flip()
print(p)

dfs <- subset(df.preprocessed, is.na(pagecount.orig) & !is.na(pagecount))
p <- ggplot(dfs,
       aes(x = gatherings, y = pagecount)) +
       geom_count() +
       scale_y_log10(breaks=10^(0:n)) +
       coord_flip() +
       ggtitle(paste("Estimated page counts\n(n=", sum(is.na(df.preprocessed$pagecount.orig) & !is.na(df.preprocessed$pagecount)), ")", sep = "")) +
       xlab("Gatherings") +
       ylab("Estimated page count")
print(p)
```



## Documents with missing pages over years 

```{r missingpages, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.height=4}
library(dplyr)
df2 <- df %>% group_by(publication_decade, gatherings) %>% summarize(na = sum(is.na(pagecount.orig) | (!pagecount.orig == pagecount)))
df2$na[df2$na == 0] <- NA
library(ggplot2)
theme_set(theme_bw(15))
p <- ggplot(df2, aes(x = publication_decade, y = gatherings, size = na))
p <- p + geom_point()
p <- p + xlim(c(min(na.omit(df$publication_decade)), 1800))
p <- p + ggtitle(paste("Documents with missing page counts (original; n=", sum(na.omit(df2$na)), ")", sep = ""))
p <- p + xlab("Publication decade")
p <- p + ylab("Document dimension (estimated)")
print(p)

library(dplyr)
df2b <- df %>% group_by(publication_decade, gatherings) %>% summarize(na = sum(is.na(pagecount)))
df2b$na[df2b$na == 0] <- NA
library(ggplot2)
theme_set(theme_bw(15))
p <- ggplot(df2b, aes(x = publication_decade, y = gatherings, size = na))
p <- p + geom_point()
p <- p + xlim(c(min(na.omit(df$publication_decade)), 1800))
p <- p + ggtitle(paste("Documents with missing page counts (after adding estimates; n=", sum(na.omit(df2b$na)), ")", sep = ""))
p <- p + xlab("Publication decade")
p <- p + ylab("Document dimension (estimated)")
print(p)
```


## Estimated paper consumption

Note: there are `r length(which(is.na(df$area) & (!is.na(df$gatherings.original) | !is.na(df$width.original) | !is.na(df$height.original))))` documents that have some dimension info but sheet area information could not be calculated. 

```{r paperconsumption, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.height=5}
library(dplyr)
library(ggplot2)
df <- df.preprocessed
df2 <- df %>% group_by(publication_year) %>% summarize(paper = sum(paper, na.rm = TRUE), n = n()) 
library(sorvi)
p <- regression_plot(paper ~ publication_year, df2) 
p <- p + ggtitle("Total annual paper consumption")
p <- p + xlab("Year")
p <- p + ylab("Paper consumption")
print(p)

p <- regression_plot(n ~ publication_year, df2) 
p <- p + ggtitle("Total annual documents")
p <- p + xlab("Year")
p <- p + ylab("Documents (n)")
print(p)
```

```{r paperconsumption2b, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.height=5}
df <- df.preprocessed
df2 <- df %>% group_by(publication_year, gatherings) %>%
              summarize(paper = sum(paper, na.rm = TRUE), n = n())
df2 <- df2 %>% filter(gatherings %in% names(which(table(df2$gatherings) >= 50)))
p <- ggplot(df2, aes(y = paper, x = publication_year, group = gatherings, color = gatherings))
p <- p + geom_point()
p <- p + geom_smooth(method = "loess", size = 1)
p <- p + ggtitle("Annual paper consumption by gatherings")
p <- p + xlab("Year")
p <- p + ylab("Paper (sheets)")
p <- p + scale_color_discrete(guide = guide_legend(title = "Doc. Size"))
p <- p + scale_y_log10()
print(p)

p <- ggplot(df2, aes(y = n, x = publication_year, group = gatherings, color = gatherings))
p <- p + geom_point()
p <- p + geom_smooth(method = "loess", size = 1)
p <- p + ggtitle("Annual title count by gatherings")
p <- p + xlab("Year")
p <- p + ylab("Documents (n)")
p <- p + scale_y_log10()
print(p)
```



```{r pagecounts-gatherings-relab, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.height=5}
# Top gatherings
top <- names(which(table(na.omit(df.preprocessed$gatherings)) >= 50))

df2 <- df.preprocessed %>%
              group_by(publication_decade, gatherings) %>%
              summarize(paper = sum(paper, na.rm = TRUE), n = n()) %>%
	      filter(!is.na(gatherings) & !gatherings == "NA")
df3 <- df2 %>% filter(gatherings %in% top)
	       
p <- ggplot(df3, aes(x = publication_decade, y = paper, fill = gatherings)) +
                 geom_bar(position = "stack", stat = "identity") +
		 ggtitle("Top gatherings paper consumption over time")
print(p)

p <- ggplot(df2, aes(x = publication_decade, y = n, fill = gatherings)) +
       geom_bar(position = "fill", stat = "identity") +
       ggtitle("All gatherings count proportions over time") +
       ylab("Fraction (%)") + scale_y_continuous(labels = scales::percent)
print(p)    
```

```{r paperconsumption2, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=13, fig.height=6}
df2 <- df.preprocessed %>%
              group_by(publication_decade) %>%
              summarize(paper = sum(paper, na.rm = TRUE), n = n()) 
p <- ggplot(df2, aes(x = publication_decade, y = paper/n))
p <- p + geom_point()
p <- p + geom_line()
p <- p + ggtitle("Average paper consumption per document")
p <- p + xlab("Year")
p <- p + scale_y_log10()
p <- p + ylab("Paper (sheets)")
print(p)
```



## Pamphlets vs. Books

```{r doctypes, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=13, fig.height=6}
library(ggplot2)
library(dplyr)

df <- df.preprocessed
df$type <- rep(NA, nrow(df))
df$type[df$pagecount > 32] <- "book"
df$type[df$pagecount <= 32] <- "pamphlet"
df$type <- factor(df$type)

df2 <- df %>% group_by(publication_year, type) %>%
              summarize(paper = sum(paper, na.rm = TRUE), n = n()) %>%
	      filter(!is.na(type))
p <- ggplot(df2, aes(x = publication_year, y = paper, group = type, color = type))
p <- p + geom_point()
p <- p + geom_smooth(method = "loess")
p <- p + ggtitle("Paper consumption per document type")
p <- p + xlab("Year")
p <- p + scale_y_log10()
p <- p + ylab("Paper (sheets)")
print(p)

p <- ggplot(df2, aes(x = publication_year, y = n, group = type, color = type))
p <- p + geom_point()
p <- p + geom_smooth(method = "loess")
p <- p + ggtitle("Documents per document type")
p <- p + xlab("Year")
p <- p + scale_y_log10()
p <- p + ylab("Documents (n)")
print(p)
```


```{r doctypes2, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=13, fig.height=6}
df <- df.preprocessed
df <- mutate(df, length = cut(pagecount, c(0, 32, 72, 148, 250, 450, 800, 2000, 5000, Inf)))

df2 <- df %>% group_by(publication_decade, length) %>%
              summarize(paper = sum(paper, na.rm = TRUE), n = n()) %>%
	      filter(!is.na(paper))
	      
p <- ggplot(df2, aes(x = publication_decade, y = paper, group = length, color = length))
p <- p + geom_point()
p <- p + geom_smooth(method = "loess", size = 1)
p <- p + ggtitle("Paper consumption per document type")
p <- p + xlab("Decade")
p <- p + ylab("Paper (sheets)")
p <- p + scale_y_log10()
print(p)

p <- ggplot(df2, aes(x = publication_decade, y = n, group = length, color = length))
p <- p + geom_point()
p <- p + geom_smooth(method = "loess", size = 1)
p <- p + ggtitle("Documents per document type")
p <- p + xlab("Publication time")
p <- p + ylab("Documents (n)")
p <- p + scale_y_log10()
print(p)
```



## Nature of the documents over time

Estimated paper consumption by document size

```{r 20150611paris-paper6, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=13, fig.height=7}
df <- df.preprocessed
df2 <- df %>% group_by(publication_year, gatherings) %>%
              summarize(paper = sum(paper, na.rm = TRUE), n = n()) 
df2 <- filter(df2, gatherings %in% names(which(table(df2$gatherings) >= 50)))
p <- ggplot(df2, aes(y = paper, x = publication_year, group = gatherings, color = gatherings))
p <- p + geom_point()
p <- p + geom_smooth(method = "loess", size = 1)
p <- p + ggtitle("Annual paper consumption by gatherings")
p <- p + xlab("Year")
p <- p + scale_y_log10()
p <- p + ylab("Paper consumption")
print(p)
```


Gatherings height: does it change over time? How increased printing activity is related to book size trends? Alternatively, we could use area (height x width), or median over time. Note that only original (not augmented) dimension info is being used here.

```{r pagecounts-gatsize, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.height=5}
df <- df.preprocessed
for (g in c("1to", "2fo", "4to", "8vo")) {
  df2 <- filter(df,
       		  !is.na(gatherings.original) &
		  !is.na(height.original) &
		  gatherings.original == g) %>%
       group_by(publication_year, height.original) %>%
       tally()

  p <- ggplot(df2, aes(x = publication_year, y = height.original)) + geom_point(aes(size = n)) 
  p <- p + ggtitle(paste(g, "heights over time"))
  p <- p + xlab("Publication time")
  print(p)
}
```


Page counts: does it change over time? Also suggested we could calculate some kind of factor for each time period based on this ? In principle, we could calculate this separately for any given publication place as well but letÍ„s discuss this later. Would help to specify some specific places of interest.

```{r pagecounts-gatsize2, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.height=5}
df <- df.preprocessed
for (g in c("1to", "2fo", "4to", "8vo")) {
  df2 <- filter(df,
       		  !is.na(gatherings.original) &
		  !is.na(pagecount.orig) &
		  gatherings.original == g)
  df3 <- df2 %>%
       group_by(publication_year, pagecount.orig) %>%
       tally()

  p <- ggplot(df3, aes(x = publication_year, y = pagecount.orig)) + geom_point(aes(size = n))
  p <- p + geom_smooth(data = df2, aes(x = publication_year, y = pagecount.orig))
  p <- p + ggtitle(paste(g, "pagecount over time"))
  p <- p + xlab("Publication time")  
  print(p)
}
```


Same for documents that have a sufficient number of pages:

```{r pagecounts-gatsize3, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10, fig.height=5}
df <- df.preprocessed
minpages <- 60
for (g in c("1to", "2fo", "4to", "8vo")) {
  df2 <- filter(df, pagecount >= minpages &
       		  !is.na(gatherings.original) &
		  !is.na(pagecount.orig) &
		  gatherings.original == g)
  df3 <- df2 %>%
       group_by(publication_year, pagecount.orig) %>%
       tally()

  p <- ggplot(df3, aes(x = publication_year, y = pagecount.orig)) + geom_point(aes(size = n))
  p <- p + geom_smooth(data = df2, aes(x = publication_year, y = pagecount.orig))
  p <- p + ggtitle(paste(g, "pagecount over time (books with over", minpages, "pages)"))
  print(p)
}
```

